{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 - Using Feed-Forward Neural Networks for linear and logistic regression\n",
    "## **Part b)** - Feed-Forward Neural Network\n",
    "\n",
    "#### Program imports and defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkClasses import *\n",
    "from classSupport import *\n",
    "from methodSupport import *\n",
    "\n",
    "import autograd.numpy as anp\n",
    "from autograd import grad,elementwise_grad\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Random seed\n",
    "def_seed = 1\n",
    "np.random.seed(def_seed); anp.random.seed(def_seed)\n",
    "\n",
    "## Figure defaults\n",
    "plt.rcParams[\"figure.figsize\"] = (8,3); plt.rcParams[\"font.size\"] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anp.random.seed(def_seed)\n",
    "cases = ['1D','2D','Franke']\n",
    "case_ = cases[1]\n",
    "show = True #True False\n",
    "\n",
    "# Grid and data setup\n",
    "a      = [1.0, 1.5, 1.2]                                   # Coefficients for exponential model\n",
    "c0, c1 = 0.2, 0.95                                         # Noise scaling    \n",
    "x0, xN = 0, 1                                              # Start and end of domain, x-axis\n",
    "y0, yN = 0, 1                                              # Start and end of domain, y-axis\n",
    "Nx, Ny = 100, 100                                            # Number of sample points\n",
    "\n",
    "dataset = Initializer(problem_case=case_,sample_size=[Nx,Ny])\n",
    "dataset.domain_setup(noise=c0)\n",
    "dataset.test_function(a)\n",
    "if case_ == '1D':\n",
    "    f = dataset.plot(labels=['Test function','x','y','dataset','true'])\n",
    "else:\n",
    "    f = dataset.plot(labels=['Test function','X','Y','Z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Neural Network analysis\n",
    "#### Training network with fixed parameters $\\eta, \\gamma, \\lambda$\n",
    "Simple training of the network with fixed parameters for the learning rate $\\eta$, regularization, $\\lambda$, gradient momentum, $\\gamma$, as well as a fixed number of mini-batches and epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anp.random.seed(def_seed)\n",
    "\n",
    "if case_ == '1D':\n",
    "    target = dataset.target[0]\n",
    "    input = dataset.x\n",
    "\n",
    "else:\n",
    "    target = dataset.target_f\n",
    "    x = dataset.xf; y = dataset.yf\n",
    "    input = anp.zeros((x.shape[0],2))\n",
    "    input[:,0] = x[:,0]\n",
    "    input[:,1] = y[:,0] \n",
    "\n",
    "n_inputs,n_features = input.shape\n",
    "\n",
    "layer_output_sizes = [5,100,5,1]\n",
    "\n",
    "hidden_func  = ReLU #sigmoid ReLU, expReLU, LeakyReLU,identity\n",
    "hidden_der = elementwise_grad(hidden_func,0)\n",
    "\n",
    "act_funcs = []; act_der = []\n",
    "for i in range(len(layer_output_sizes)-1):\n",
    "    act_funcs.append(hidden_func)\n",
    "    act_der.append(hidden_der)\n",
    "act_funcs.append(identity); \n",
    "output_der = elementwise_grad(act_funcs[-1],0); act_der.append(output_der)\n",
    "\n",
    "cost_func = mse_predict\n",
    "cost_der   = grad(cost_func,0)\n",
    "\n",
    "network = FFNeuralNework(network_input_size=n_features,layer_output_size=layer_output_sizes,\n",
    "                         activation_functions=act_funcs,activation_derivatives=act_der,\n",
    "                         cost_function=cost_func,cost_derivative=cost_der)\n",
    "network.reset()\n",
    "network.create_layers()\n",
    "\n",
    "## Gradient Descent setup\n",
    "eta = 0.001\n",
    "gamma = 0.00000001\n",
    "lmbda = 0.0001; lp = 2\n",
    "batches = 30; epoch = 200\n",
    "decay_rms = 0.9\n",
    "adagrad_mom = 0.00000000001\n",
    "ADAM_decay = [0.9, 0.99]\n",
    "\n",
    "## Calling the gradient descent (GD)-method\n",
    "#GDMethod = [PlainGD(eta,lmbda=lmbda,lp=lp),PlainGD(eta,lmbda=lmbda,lp=lp)]\n",
    "GDMethod = [MomentumGD(eta,gamma,lmbda=lmbda,lp=lp),MomentumGD(eta,gamma,lmbda=lmbda,lp=lp)]\n",
    "#GDMethod = [Adagrad(eta,adagrad_mom,lmbda=lmbda,lp=lp),Adagrad(eta,adagrad_mom,lmbda=lmbda,lp=lp)] \n",
    "#GDMethod = [RMSprop(eta,decay=decay_rms),RMSprop(eta,decay_rms)] \n",
    "#GDMethod = [ADAM(eta,ADAM_decay),ADAM(eta,ADAM_decay)] \n",
    "\n",
    "network.train_network(input,target,GDMethod,batches=batches,epochs=epoch)\n",
    "\n",
    "final_predict = network.feed_forward(input)\n",
    "\n",
    "if show == True:\n",
    "    print(f'Method: {GDMethod[0].__class__.__name__}')\n",
    "    print('Regularization, λ:',lmbda)\n",
    "    print('Learning rate,  η:',eta)\n",
    "    print('MSE: ',mse_predict(final_predict,target))\n",
    "\n",
    "if case_ == '1D':\n",
    "    plot1D(input,[target,final_predict],\n",
    "           labels=[f'Network prediction\\nGD-method: {GDMethod[0].__class__.__name__}'\n",
    "                                                ,'x','y','f (x)','ỹ (x)','',''])\n",
    "else:\n",
    "    final_fit = final_predict.reshape(Nx,Ny)\n",
    "    plot2D(dataset.xx,dataset.yy,final_fit,\n",
    "           labels=[f'Network prediction\\nGD-method: {GDMethod[0].__class__.__name__}'\n",
    "                                                   ,'X','Y','Z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study of regularization, $\\lambda$ vs. learning rate, $\\eta$\n",
    "Keeping the gradient descent momentum, $\\gamma$, number of mini-batches and epochs fixed, this shows the training of the network with different values for the learning rate, $\\eta$, and regularization term, $\\lambda$. The `PlainGD`-method is a regularized stochastic gradient descent method if $\\lambda = 0$, where the regularization order is set with the `lp`-keyword. The same holds for the `MomentumGD`-method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anp.random.seed(def_seed)\n",
    "\n",
    "## Resetting some instance variables of the network before next run\n",
    "network.reset()\n",
    "\n",
    "## \n",
    "show = False\n",
    "\n",
    "## SDG-parameters\n",
    "num_param = 4\n",
    "eta = anp.logspace(-4,0,num_param)\n",
    "lmbda = anp.logspace(-10,-2,num_param)\n",
    "gamma = 0.00001; lp = 2\n",
    "adagrad_gamma = gamma\n",
    "\n",
    "MSE_NN = anp.zeros((len(eta),len(lmbda)))\n",
    "for i in range(len(eta)):\n",
    "    for j in range(len(lmbda)):\n",
    "        ## Recreating the layers\n",
    "        network.create_layers()\n",
    "\n",
    "        ## Calling GDMethod with new parameters\n",
    "        #GDMethod = [PlainGD(eta[i],lmbda=lmbda[j],lp=lp),PlainGD(eta[i],lmbda=lmbda[j],lp=lp)]\n",
    "        GDMethod = [MomentumGD(eta[i],gamma,lmbda=lmbda[j],lp=lp),MomentumGD(eta[i],gamma,lmbda=lmbda[j],lp=lp)]\n",
    "        #GDMethod = [Adagrad(eta[i],adagrad_mom,lmbda=lmbda,lp=lp),Adagrad(eta[i],adagrad_mom,lmbda=lmbda,lp=lp)] #\n",
    "\n",
    "        network.train_network(input,target,GDMethod,batches=batches,epochs=epoch)\n",
    "\n",
    "        final_predict = network.feed_forward(input)\n",
    "        \n",
    "        MSE_NN[i,j] = mse_predict(final_predict,target)\n",
    "        if show == True:\n",
    "            print(f'Method: {GDMethod[0].__class__.__name__}')\n",
    "            print('Regularization, λ:',lmbda[j])\n",
    "            print('Learning rate,  η:',eta[i])\n",
    "            print('MSE: ',mse_predict(final_predict,target))\n",
    "\n",
    "    network.reset()\n",
    "\n",
    "#MSE_NN = anp.nan_to_num(MSE_NN,nan=1)\n",
    "\n",
    "fig,ax = lambda_eta(MSE_NN,[lmbda,eta],\n",
    "                    axis_tick_labels=[[f'{y:.0e}' for y in lmbda],[f'{y:.0e}' for y in eta]],\n",
    "                    cbar_lim=[0.,2.]\n",
    "                    )\n",
    "_ = ax.set_title(f'{GDMethod[0].__class__.__name__}: MSE for λ vs. η')#\\nγ = %.0e'%(gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study of momentum, $\\gamma$ vs. learning rate, $\\eta$\n",
    "Keeping the regularization parameter, $\\lambda$, number of mini-batches and epochs fixed, this studies how the different values of learning rate, $\\eta$, and gradient descent momentum, $\\gamma$, affects the network training. The `PlainGD`-method is a regularized stochastic gradient descent method if $\\lambda = 0$, where the regularization order is set with the `lp`-keyword. The same holds for the `MomentumGD`-method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anp.random.seed(def_seed)\n",
    "\n",
    "network.reset()\n",
    "\n",
    "#num_param = 4\n",
    "eta = anp.logspace(-6,-2,num_param)\n",
    "gamma = anp.logspace(-10,-6,num_param)\n",
    "lmbda = 1e-7; lp = 2\n",
    "#batches = 40; epoch = 100\n",
    "\n",
    "MSE_NN = anp.zeros((len(eta),len(gamma)))\n",
    "for i in range(len(eta)):\n",
    "    for j in range(len(gamma)):\n",
    "        network.create_layers()\n",
    "        #GDMethod = [PlainGD(eta[i],lmbda=lmbda[j]),PlainGD(eta[i],lmbda=lmbda[j])]\n",
    "        GDMethod = [MomentumGD(eta[i],gamma[j],lmbda=lmbda,lp=lp),MomentumGD(eta[i],gamma[j],lmbda=lmbda,lp=lp)]\n",
    "        #GDMethod = [Adagrad(eta,adagrad_mom),Adagrad(eta,adagrad_mom)] #\n",
    "        #GDMethod = [RMSprop(eta,decay=decay_rms),RMSprop(eta,decay_rms)] #\n",
    "        #GDMethod = [ADAM(eta,ADAM_decay),ADAM(eta,ADAM_decay)] #\n",
    "\n",
    "        network.train_network(input,target,GDMethod,batches=batches,epochs=epoch)\n",
    "\n",
    "        final_predict = network.feed_forward(input)\n",
    "        \n",
    "        MSE_NN[i,j] = mse_predict(final_predict,target)\n",
    "        if show == True:\n",
    "            print(f'Method: {GDMethod[0].__class__.__name__}')\n",
    "            print('Momentum,       γ:',gamma[j])\n",
    "            print('Learning rate,  η:',eta[i])\n",
    "            print('MSE: ',mse_predict(final_predict,target))\n",
    "\n",
    "    network.reset()\n",
    "\n",
    "fig,ax = lambda_eta(MSE_NN,[gamma,eta],\n",
    "                    axis_tick_labels=[[f'{y:.0e}' for y in gamma],[f'{y:.0e}' for y in eta]],\n",
    "                    cbar_lim=[0.,2.]\n",
    "                    )\n",
    "ax.set_title(f'{GDMethod[0].__class__.__name__}: MSE for γ vs. η\\nλ = %.0e'%(lmbda))\n",
    "_ = ax.set_xlabel('γ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study of Adagrad momentum, $\\gamma_{\\text{adagrad}}$, and regularization, $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anp.random.seed(def_seed)\n",
    "\n",
    "network.reset()\n",
    "\n",
    "#num_param = 4\n",
    "eta = 1e-5\n",
    "adagrad_gamma = anp.logspace(-12,-6,num_param)\n",
    "lmbda = anp.logspace(-10,-6,num_param)\n",
    "lp = 2\n",
    "#batches = 40; epoch = 100\n",
    "\n",
    "MSE_NN = anp.zeros((len(lmbda),len(adagrad_gamma)))\n",
    "for i in range(len(lmbda)):\n",
    "    for j in range(len(adagrad_gamma)):\n",
    "        network.create_layers()\n",
    "\n",
    "        GDMethod = [Adagrad(eta,adagrad_gamma[j],lmbda=lmbda[i],lp=lp),Adagrad(eta,adagrad_gamma[j],lmbda=lmbda[i],lp=lp)]\n",
    "\n",
    "        network.train_network(input,target,GDMethod,batches=batches,epochs=epoch)\n",
    "\n",
    "        final_predict = network.feed_forward(input)\n",
    "        \n",
    "        MSE_NN[i,j] = mse_predict(final_predict,target)\n",
    "        if show == True:\n",
    "            print(f'Method: {GDMethod[0].__class__.__name__}')\n",
    "            print('Momentum,       γ:',gamma[j])\n",
    "            print('Learning rate,  η:',eta[i])\n",
    "            print('MSE: ',mse_predict(final_predict,target))\n",
    "\n",
    "    network.reset()\n",
    "\n",
    "fig,ax = lambda_eta(MSE_NN,[adagrad_gamma,lmbda],\n",
    "                    axis_tick_labels=[[f'{y:.0e}' for y in adagrad_gamma],[f'{y:.0e}' for y in lmbda]],\n",
    "                    cbar_lim=[0.,2.]\n",
    "                    )\n",
    "ax.set_title(f'{GDMethod[0].__class__.__name__}: MSE for γ vs. λ\\nη = %.0e'%(eta))\n",
    "_ = ax.set_xlabel('γ'); _ = ax.set_ylabel('λ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Scikit-Learn's Neural network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "lmbda = 1e-4; gamma = 1e-3\n",
    "sklearn_network = MLPRegressor(layer_output_sizes,activation='relu',solver='sgd',batch_size=5,\n",
    "                               max_iter=1000,warm_start=True,alpha=lmbda,\n",
    "                               learning_rate_init=eta,\n",
    "                               random_state=def_seed,momentum=gamma\n",
    "                               )\n",
    "\n",
    "sklearn_network.fit(input,target)\n",
    "final_pred = sklearn_network.predict(input)\n",
    "print(sklearn_network.score(input,target))\n",
    "print(sklearn_network.best_loss_)\n",
    "print(sklearn_network.loss_)\n",
    "if case_ == '1D':\n",
    "    plot1D(input,[target,final_pred],\n",
    "           labels=[f'Network prediction\\nScikit-learn'\n",
    "                                                ,'x','y','f (x)','ỹ (x)','',''])\n",
    "else:\n",
    "    final_pred = final_pred.reshape(Nx,Ny)\n",
    "    plot2D(dataset.xx,dataset.yy,final_pred,\n",
    "           labels=[f'Network prediction\\nScikit-learn'\n",
    "                                                   ,'X','Y','Z'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p2-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
